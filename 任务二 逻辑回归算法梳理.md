#### 1. 逻辑回归与线性回归的联系与区别

[参考https://blog.csdn.net/lx_ros/article/details/81263209](https://blog.csdn.net/lx_ros/article/details/81263209)

逻辑回归与线性回归都属于广义线性回归模型,其区别与联系从以下几个方面进行比较
1. 分类与回归: 回归模型就是预测一个连续变量,在分类问题中,预测属于某类的概率,可以看成回归问题。这可以说是使用回归算法的分类算法。
2. 输出:直接使用线性回归的输出作为概率是有问题的,因为其值有可能小于0或者大于1,这是不符合实际情况的,逻辑回归的输出正是[0,1]区间。
3. 参数估计方法:线性回归中使用的是最小化平方误差损失函数,对偏离真实值越远的数据惩罚越严重,而逻辑回归使用的对数似然函数进行参数估计,使用交叉熵作为损失函数,对预测错误的惩罚是随着输出的增大,逐渐逼近一个常数,也正是因为使用的参数估计的方法不同，线性回归模型更容易受到异常值(outlier)的影响，有可能需要不断变换阈值(threshold),使用逻辑回归的方法进行分类,就明显对异常值有较好的稳定性。
4. 参数解释: 线性回归中，独立变量的系数解释十分明了，就是保持其他变量不变时，改变单个变量因变量的改变量。而逻辑回归中,自变量系数的解释就要视情况而定了，要看选用的概率分布是什么，如二项式分布，泊松分布等


#### 2. 逻辑回归的原理
逻辑回归是一个分类算法,它是在线性回归的基础上加入了一个sigmoid函数,将线性回归的结果输入至sigmoid函数中,并且设定一个阈值,如果大于阈值为1,小于阈值为0

#### 3.逻辑回归损失函数推导及优化
[参考链接https://blog.csdn.net/fu_9701/article/details/83097176](https://blog.csdn.net/fu_9701/article/details/83097176)

#### 4.正则化与模型评估指标
正则化: 简单来说,我们在模型的拟合过程中,为了尽可能的减少成本函数,让模型对训练数据更好的拟合,可能倾向于选择增加模型的参数,这样就可能会导致过拟合。正则化的方式则是选择在成本函数中增加一个正则化项,这个项会使得成本函数随着模型参数的增加而变大,从而在最小化成本函数的过程中,倾向模型参数较少的模型,平衡拟合程度和模型复杂度来避免过拟合问题。

模型评估指标:
1. 准确率:预测正确个数占总数的比例
   精准率:正例样本中有多少被预测正确了
   召回率:预测正例样本中有多少是正确的
2. ROC曲线:显示的是对超出限定阈值的所有预测结果的分类效果。


#### 5.逻辑回归的优缺点
[参考https://blog.csdn.net/mr_hhh/article/details/79433094](https://blog.csdn.net/mr_hhh/article/details/79433094)

优点:
1. 预测结果是介于0和1之间的概率
2. 可以适用于连续性和类别性自变量
3. 容易使用和解释

缺点:
1）对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；

2）预测结果呈“S”型，因此从log(odds)向概率转化的过程是非线性的，在两端随着log(odds)值的变化，概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。


#### 6.样本不均衡问题解决办法
[参考https://blog.csdn.net/zhongjunlang/article/details/79568601](https://blog.csdn.net/zhongjunlang/article/details/79568601)
1. 扩充数据集
2. 对数据集进行重采样
3. 人造数据
4. 改变分类算法
5. 尝试其他的分类指标


#### 7.sklearn参数
[参考https://blog.csdn.net/qq_38683692/article/details/82533460](https://blog.csdn.net/qq_38683692/article/details/82533460)

