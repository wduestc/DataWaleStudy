#### 1. 信息论基础
熵: 表示一个随机变量的复杂性或者不确定性

联合熵: 一集变量之间不确定性的衡量手段

条件熵:表示在直到某一个条件后,某一随机变量的复杂性或不确定性

信息增益:以某特征划分数据集前后的熵的差值

基尼不纯度: 将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率

#### 2.决策树的不同分类算法的原理及应用场景

ID3算法: 对当前例子集合，计算属性的信息增益；选择信息增益最大的属性Ai(关于信息增益后面会有详细叙述)；把在Ai处取值相同的例子归于同于子集，Ai取几个值就得几个子集；对依次对每种取值情况下的子集,递归调用建树算法，即返回a；若子集只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处。

C4.5:  
1.计算每个特征的信息增益率  
2.选择增益率最高的特征,根据其特质值进行划分  
3.去除上步被选择的特征，在每个分支数据集中，重复1，2，3直到数据集不能或不用再次划分

CART: 采用基尼系统划分数据

#### 3.回归树原理
[参考https://blog.csdn.net/u012835414/article/details/86021339#3.%20%E5%9B%9E%E5%BD%92%E6%A0%91%E5%8E%9F%E7%90%86](https://blog.csdn.net/u012835414/article/details/86021339#3.%20%E5%9B%9E%E5%BD%92%E6%A0%91%E5%8E%9F%E7%90%86)  
回归是为了处理预测值是连续分布的情景，其返回值应该是一个具体预测值。回归树的叶子是一个个具体的值，从预测值连续这个意义上严格来说，回归树不能称之为“回归算法”。因为回归树返回的是“一团”数据的均值，而不是具体的、连续的预测值（即训练数据的标签值虽然是连续的，但回归树的预测值却只能是离散的）。所以回归树其实也可以算为“分类”算法，其适用场景要具备“物以类聚”的特点，即特征值的组合会使标签属于某一个“群落”，群落之间会有相对鲜明的“鸿沟”。利用回归树可以将复杂的训练数据划分成一个个相对简单的群落，群落上可以再利用别的机器学习模型再学习。
模型树的叶子是一个个机器学习模型，如线性回归模型，所以更称的上是“回归”算法。利用模型树就可以度量一个人的文艺值了。
回归树和模型树也需要剪枝,剪枝理论和分类树相同。为了获得最佳模型，树剪枝常采用预剪枝和后剪枝结合的方法进行。

#### 4.决策树防止过拟合手段
1.合理、有效地抽样，用相对能够反映业务逻辑的训练集去产生决策树
2.提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝。

#### 5. 模型评估
平均绝对误差、均方误差、R-squared

#### 6.sklearn参数详解
DecisionTreeClassifier(criterion="gini",splitter="best",max_depth=None,min_samples_split=2,min_samples_leaf=1, min_weight_fraction_leaf=0.,max_features=None,random_state=None,max_leaf_nodes=None, min_impurity_decrease=0, min_impurity_split=None,class_weight=None,presort=False)

criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。  
splitter:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。  
max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。  
min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。  
min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。  
min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。  
max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。  
random_state:随机种子的设置，与LR中参数一致。  
max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。  
min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。  
min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。  
class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。  
presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。  