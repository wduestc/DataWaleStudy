#### 1. 机器学习的一些概念

有监督: 学习一个模型,使模型能够对任意给定的输入,对其相应的输出做出一个好的预测。

无监督: 相对于监督学习,即不知道数据集中数据、特征之间的关系,而是要根据聚类或一定的模型得到数据之间的关系。

泛化能力: 是指机器学习算法对新鲜样本的适应能力,学习的目的是学到隐含在数据背后的规律,对具有同一规律的学习集以外的数据,经过训练的网络也能给出合适的输出,该能力称为泛化能力。

过拟合: 为了得到一致假设而使假设变得过度复杂称为过拟合。

欠拟合: 模型拟合不够,在训练集上表现效果差,没有充分利用数据,预测准确度低。

偏差: 反映的是模型在样本上的输出与真实值之间的误差。

方差: 反映的是模型每一次输出结果与模型输出期望之间的误差。

防止过拟合的方法: 1. 获取更多的数据 2.减少特征变量 3.正则化 4.结合多种模型

防止欠拟合的方法: 1. 引入新的特征 2.添加多项式特征 3.减少正则化参数。

交叉验证: 重复的使用数据,把给定的数据进行切分,将切分的数据集组合为训练集与测试集,在此基础上反复地进行训练、测试以及模型选择。

#### 2. 线性回归的原理

线性回归是利用数理统计中的回归分析,来确定两种或两种以上的变数间相互依赖的定量关系的一种统计分析方法之一,运用十分广泛。

分析按照自变量和因变量之间的关系类型,可分为线性回归分析和非线性回归分析。

如果在回归分析中,只包含一个自变量和一个因变量,且二者的关系可用一条直线近似表示,这种回归分析称为一元线性回归分析,如果回归分析中包括两个或两个以上的自变量,且因变量和自变量之间是线性关系,则称为多元线性回归分析。

#### 3. 线性回归损失函数、代价函数、目标函数

损失函数:  定义在单个样本上,算是一个样本的误差,表示每个训练数据点到拟合直线的竖值距离的平方和

代价函数: 是定义在整个训练集上的,是所有样本误差的平均,也就是损失函数的平均。

目标函数: 定义为最终需要优化的函数,等于经验风险+结构风险(也就是损失函数+正则化项)

#### 4. 优化方法(梯度下降法、牛顿法、拟牛顿法)

梯度下降法: 是一种常用的一阶优化方法,是求解无约束优化问题的经典方法,通过反复迭代,沿目标函数梯度的反方向逼近目标函数的最优解。

牛顿法、拟牛顿法:
是常用的二阶优化方法,是求解无约束优化问题的经典方法。由于用到了二阶导数信息,因此相比梯度下降法收敛速度更快。牛顿法也是通过反复迭代,求解目标函数的最优解,但是牛顿法每次迭代需要求解目标函数的海塞矩阵的逆矩阵,因此计算相对复杂。拟牛顿法通过正定矩阵近似海塞矩阵的逆矩阵或者海塞矩阵,简化了计算过程。

#### 5. 线性回归的评估指标

残差估计: 总体思想是计算实际值与预测值间的差值简称残差。从而实现对回归模型的评估,一般可以画出残差图,进行分析评估、估计模型的异常值、同时还可以检查模型是否是线性的、以及误差是否随机分布。

均方误差: 均方误差是线性模型拟合过程中,最小化误差平方和代价函数的平均值。

决定系数: 可以看做是均方误差的标准化版本,用于更好的解释模型的性能。

#### 6. sklearn参数详解
```
sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)

- fit_intercept:模型是否存在截距
- normalize: 模型是否对数据归一化
- copy_X: 默认为True, X会被copied,否则X将会被覆写
- n_jobs: 默认为1。计算时使用的核数。
```